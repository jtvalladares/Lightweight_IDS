{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 11:24:01.197502: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 11:24:01.218914: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 11:24:01.322807: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 11:24:01.322848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 11:24:01.341182: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 11:24:01.381931: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 11:24:01.382943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 11:24:02.030375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(X, threshold):\n",
    "    # Eliminar columnas constantes\n",
    "    X = X.loc[:, X.apply(pd.Series.nunique) > 1]\n",
    "    # Calcular la matriz de correlación absoluta\n",
    "    corr_matrix = X.corr().abs()\n",
    "    # Seleccionar la parte superior de la matriz de correlación\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Identificar las columnas a eliminar para evitar la correlación alta\n",
    "    to_drop = set()\n",
    "    for column in upper.columns:\n",
    "        # Obtener las columnas correlacionadas con la actual\n",
    "        correlated_columns = [col for col in upper.columns if upper[col][column] > threshold]\n",
    "        # Si hay columnas correlacionadas, eliminar todas excepto la primera\n",
    "        if correlated_columns:\n",
    "            to_drop.update(correlated_columns[1:])\n",
    "\n",
    "    # Eliminar las columnas correlacionadas\n",
    "    return X.drop(columns=list(to_drop), errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_features(features_array):\n",
    "# seleccionar los m features que mas se repitan en cada experimento\n",
    "    flattened = np.concatenate([np.unique(row) for row in features_array])\n",
    "    unique_feats, counts = np.unique(flattened, return_counts=True)\n",
    "    mean_feats = sum(len(row) for row in features_array) / len(features_array)\n",
    "    print('Mean features:', mean_feats)\n",
    "\n",
    "    sorted_indices = np.argsort(-counts)  # Índices ordenados por frecuencia (descendente)\n",
    "    freq_feat = unique_feats[sorted_indices][:int(np.ceil(mean_feats))]\n",
    "\n",
    "    return freq_feat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(num_entradas, num_clases):\n",
    "    model = Sequential()\n",
    "    # Capa de entrada\n",
    "    model.add(InputLayer(input_shape=(num_entradas,)))\n",
    "    # Capas ocultas\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    # Capa de salida\n",
    "    if num_clases == 2:\n",
    "        # Para clasificación binaria\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        # Para clasificación multiclase\n",
    "        model.add(Dense(num_clases, activation='softmax'))\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='RMSprop', loss=loss, metrics=['accuracy'])\n",
    "    # devolver modelo compilado\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_1d(n_features, num_clases):\n",
    "    model = Sequential()\n",
    "    # Capa de entrada\n",
    "    # model.add(InputLayer(input_shape=(input_shape,)))\n",
    "    # Capa de entrada y primeras capas convolucionales 1D\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_features, 1)))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "\n",
    "    # Aplanar el volumen para conectarlo con capas densas\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Capas densas\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "\n",
    "    # Capa de salida\n",
    "    if num_clases == 2:\n",
    "        # Para clasificación binaria\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        # Para clasificación multiclase\n",
    "        model.add(Dense(num_clases, activation='softmax'))\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='RMSprop', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_true, y_pred):\n",
    "    print('Accuracy:  ', accuracy_score(y_true, y_pred))\n",
    "    print('Precision: ', precision_score(y_true, y_pred, average='macro'))\n",
    "    print('Recall:    ', recall_score(y_true, y_pred, average='macro'))\n",
    "    print('F1:        ', f1_score(y_true, y_pred, average='macro'))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=None)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('X-IIoTID_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class1'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:  E1.1__results.pkl\n",
      "Classes:  class1\n",
      "Training | Exp. E1.1__results.pkl | Classes class1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(x_tr, y_tr, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m## Exportar modelo ##\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(classifier, DecisionTreeClassifier):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Guardar modelo scikit-learn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "exp_names = ['E1.1__results.pkl','E1.2__results.pkl','E1.3__results.pkl','E1.4__results.pkl',\n",
    "        'E2.1__results.pkl','E2.2__results.pkl','E2.3__results.pkl','E2.4__results.pkl',\n",
    "        'E3.1__results.pkl','E3.2__results.pkl','E3.3__results.pkl','E3.4__results.pkl']\n",
    "\n",
    "keys = ['class1', 'class2', 'class3']\n",
    "exclude_cols = ['class1', 'class2', 'class3', 'Date', 'Timestamp', 'Scr_IP', 'Des_IP']\n",
    "\n",
    "X_orig = data.drop(columns=exclude_cols)\n",
    "\n",
    "# iterar sobre experimentos\n",
    "for exp in exp_names:\n",
    "    print('Experiment: ', exp)\n",
    "    # Cargar el archivo .pkl\n",
    "    with open('resultados_exp/' + exp, 'rb') as file:\n",
    "        exp_results = pickle.load(file)\n",
    "\n",
    "    # iterar sobre tipos de clases\n",
    "    for key in keys:\n",
    "        print('Classes: ', key)\n",
    "        \n",
    "        ### Filtrar features ###\n",
    "        if '.3' in exp or '.4' in exp:\n",
    "            freq_feat = get_frequent_features(exp_results[key]['features']) # features frecuentes del experimento/clase\n",
    "            X_mod = X_orig.iloc[:, freq_feat] # filtar features en X_orig\n",
    "        elif '.1' in exp:\n",
    "            X_mod = X_orig\n",
    "        # dividir sets en train y test\n",
    "        x_tr, x_te, y_tr, y_te = train_test_split(\n",
    "                X_mod, data[key].reset_index(drop=True),  # Asegura que el índice sea continuo\n",
    "                test_size=0.2,\n",
    "                random_state=42,\n",
    "                stratify=data[key]\n",
    "            )\n",
    "        \n",
    "        # print('forma x_te', np.shape(x_te))\n",
    "        # print('forma y_te', np.shape(y_te))\n",
    "\n",
    "        # print(\"Valores nulos en y_te antes de escalar:\", y_te.isna().sum())\n",
    "                \n",
    "        ### eliminar columnas correlacionadas en exp x.2 ###\n",
    "        if '.2' in exp:\n",
    "            x_tr = remove_correlated_features(x_tr, 0.99)\n",
    "            x_te = x_te[x_tr.columns]\n",
    "\n",
    "        ### Escalar ### el problema es aqui\n",
    "        x_tr = scaler.fit_transform(x_tr)\n",
    "        x_te = scaler.transform(x_te)\n",
    "\n",
    "        # print(\"Valores nulos en y_te antes de concatenar:\", y_te.isna().sum())\n",
    "\n",
    "        ### Entrenar modelos ###\n",
    "        # seleccionar clasificador\n",
    "        if 'E1' in exp:\n",
    "            classifier = DecisionTreeClassifier()\n",
    "        elif 'E2' in exp:\n",
    "            classifier = dnn_model(x_tr.shape[1], y_tr.nunique())\n",
    "        elif 'E3' in exp:\n",
    "            classifier = cnn_model_1d(x_tr.shape[1], y_tr.nunique())\n",
    "        # entrenar\n",
    "        print(f'Training | Exp. {exp} | Classes {key}')\n",
    "        if 'E2' in exp or 'E3' in exp:\n",
    "            classifier.fit(x_tr, y_tr, batch_size=250, epochs=10)\n",
    "        else:\n",
    "            classifier.fit(x_tr, y_tr)\n",
    "\n",
    "        ## Exportar modelo ##\n",
    "        if isinstance(classifier, DecisionTreeClassifier):\n",
    "            # Guardar modelo scikit-learn\n",
    "            with open(f'model_{exp}_{key}.pkl', 'wb') as file:\n",
    "                pickle.dump(classifier, file)\n",
    "        elif 'E2' in exp or 'E3' in exp:\n",
    "            # Guardar modelo de redes neuronales\n",
    "            classifier.save(f'model_{exp}_{key}.h5')\n",
    "\n",
    "        ### Testing ###\n",
    "        # para comparar con jetson\n",
    "        print(f'Testing | Exp. {exp} | Classes {key}')\n",
    "        start_time = time.time()\n",
    "        if 'E2' in exp or 'E3' in exp:\n",
    "            if key == 'class3':\n",
    "                start_time = time.time()\n",
    "                y_pred = (classifier.predict(x_te) > 0.5).astype(int).flatten()\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                y_pred = np.argmax(classifier.predict(x_te), axis=-1)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            y_pred = classifier.predict(x_te)\n",
    "        test_time = time.time() - start_time        \n",
    "        print('Tiempo test: ', test_time)\n",
    "        \n",
    "        ### Guardar para pruebas en jetson ##\n",
    "        y_te = np.expand_dims(y_te, axis=1) # Asegurarse de que y_te tenga 2 dimensiones\n",
    "        test_data = np.concatenate((x_te, y_te), axis=1) # Concatenar los arrays\n",
    "\n",
    "        y_pred = np.expand_dims(y_pred, axis=1) # Asegurarse de que y_pred tenga 2 dimensiones\n",
    "        test_data = np.concatenate((test_data, y_pred), axis=1) # Concatenar los arrays\n",
    "\n",
    "        # Guardar en formato .npy\n",
    "        np.save(f'{exp}_{key}_test.npy', test_data)\n",
    "\n",
    "        # print('forma resultado', np.shape(test_data))\n",
    "        # print(test_data)\n",
    "        # ultimas_columnas = test_data[:, -2:]\n",
    "        # nans_por_columna = np.isnan(ultimas_columnas).sum(axis=0) # Contar valores NaN en cada columna\n",
    "        # print(\"Valores NaN por columna:\", nans_por_columna) # Contar valores NaN en total para las dos columnas\n",
    "      \n",
    "        show_metrics(y_te, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
